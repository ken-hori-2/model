from enum import Enum
from tkinter import FIRST
from aem import con
import numpy as np
import random

from sklearn import preprocessing
from torch import t

from math import dist
from scipy.spatial import distance
import math

# „Éô„Éº„Çπ„ÅØEnv_model_prefer.py
# prefer(ÂÑ™ÂÖà)„ÅØ„ÄÅstress -= 1 or stress = 0 „Åô„Çã„Åì„Å®„Åß„Åù„ÅÆÈÅì„ÇíÈÄ≤„Åø„ÇÑ„Åô„Åè„Åó„Å¶„ÅÑ„Åü
# N = 1 „Åå‰∏Ä„Å§„ÇÇ„Å™„ÅÑ„Å® BPlist = []„Å†„Åã„Çâ„ÄÅÊàª„ÇãÂ†¥ÊâÄ„Åå„Å™„Åè„Å¶„ÄÅÁµÇ‰∫Ü„Åó„Å¶„Åó„Åæ„ÅÜ
# Env_bp/Env_model_prob_bp_2_Edit(match+cost).py „ÅÆÊï¥ÁêÜver.
# model.py „ÅÆÊï¥ÁêÜver.

# model_edit.py „ÅÆÂàÜÂ≤ê„Ç¢„É™ver.
# model_edit_2d.py „ÅÆÊï¥ÁêÜver.
# model_edit_2d_Edit.py „ÅÆÁ∑®ÈõÜver.

# model_edit_2d_Edit_copy.py „ÅÆÊï¥ÁêÜver.

# „Åì„Çå„Åå„ÄÅÂàÜÂ≤êÂÖà„ÇÇBPLIST„Å´ËøΩÂä†„Åô„ÇãÊúÄÊñ∞ver. „ÅÆÊï¥ÁêÜver. (0817)

# Edit_Model.py -> agent„ÅÆË°åÂãïÊ±∫ÂÆö„ÅÆ„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÇíËÄÉ„Åà„Çã„Åü„ÇÅ„Å´ÂøÖË¶ÅÊúÄ‰ΩéÈôê„Å´„Åó„Åüver.


class State():

    def __init__(self, row=-1, column=-1):
        self.row = row
        self.column = column

    def __repr__(self):
        
        return "[{}, {}]".format(self.row, self.column)

    def clone(self):
        return State(self.row, self.column)

    def __hash__(self):
        return hash((self.row, self.column))

    def __eq__(self, other):
        return self.row == other.row and self.column == other.column


class Action(Enum):
    UP = 1
    DOWN = -1
    LEFT = 2
    RIGHT = -2


class Environment():

    def __init__(self, grid, NODELIST, move_prob=0.8):
        
        self.grid = grid
        self.agent_state = State()

        # Default reward is minus. Just like a poison swamp.
        # It means the agent has to reach the goal fast!
        self.default_stress = 1 # -0.04

        # Agent can move to a selected direction in move_prob.
        # It means the agent will move different direction
        # in (1 - move_prob).
        self.move_prob = move_prob
        self.reset()

        self.NODELIST = NODELIST
        self.BPLIST = []

    @property
    def row_length(self):
        return len(self.grid)

    @property
    def column_length(self):
        return len(self.grid[0])

    @property
    def actions(self):
        return [Action.UP, Action.DOWN,
                Action.LEFT, Action.RIGHT]

    @property
    def states(self):
        states = []
        for row in range(self.row_length):
            for column in range(self.column_length):
                # Block cells are not included to the state.
                if self.grid[row][column] != 9:
                    states.append(State(row, column))
        return states

    def transit_func(self, state, action):
        transition_probs = {}
        if not self.can_action_at(state):
            # Already on the terminal cell.
            return transition_probs

        opposite_direction = Action(action.value * -1)

        for a in self.actions:
            prob = 0
            if a == action:
                prob = 1 # prob = self.move_prob
            elif a != opposite_direction:
                prob = 0 # prob = (1 - self.move_prob) / 2

            next_state = self._move(state, a)
            if next_state not in transition_probs:
                transition_probs[next_state] = prob
            else:
                transition_probs[next_state] += prob

        return transition_probs

    def can_action_at(self, state):
        if self.grid[state.row][state.column] == 0:
            return True
        else:
            return False

    def _move(self, state, action):
        if not self.can_action_at(state):
            raise Exception("Can't move from here!")

        next_state = state.clone()

        # Execute an action (move).
        if action == Action.UP:
            next_state.row -= 1
        elif action == Action.DOWN:
            next_state.row += 1
        elif action == Action.LEFT:
            next_state.column -= 1
        elif action == Action.RIGHT:
            next_state.column += 1

        # Check whether a state is out of the grid.
        if not (0 <= next_state.row < self.row_length):
            next_state = state
            
        if not (0 <= next_state.column < self.column_length):
            next_state = state
            

        # Check whether the agent bumped a block cell.
        if self.grid[next_state.row][next_state.column] == 9:
            next_state = state
            

        return next_state

    def stress_func(self, state, TRIGAR, BRANCH):
       
        done = False

        # Check an attribute of next state.
        attribute = self.NODELIST[state.row][state.column]

        if TRIGAR:
            stress = -self.default_stress
        else:
            # if attribute == 1:
            #     # Get reward! and the game ends.
            #     stress = 0 # -1 # 0                              # „Åì„Åì„Åå reward = None „ÅÆÂéüÂõ† or grid „ÅÆ 1->0 „ÅßËß£Ê±∫
            # elif attribute == 0:
            #     # Get damage! and the game ends.
            #     stress = self.default_stress
            if attribute > 0.0:
                # Get reward! and the game ends.
                stress = 0 # -1 # 0                              # „Åì„Åì„Åå reward = None „ÅÆÂéüÂõ† or grid „ÅÆ 1->0 „ÅßËß£Ê±∫
            else:
                stress = self.default_stress


        return stress, done

    def reset(self):
        # Locate the agent at lower left corner.
        # self.agent_state = State(self.row_length - 1, 0)
        self.agent_state = State(self.row_length - 2, 2)
        return self.agent_state

    def step(self, action, TRIGAR, BRANCH):
        next_state, stress, done = self.transit(self.agent_state, action, TRIGAR, BRANCH)
        if next_state is not None:
            self.agent_state = next_state
        

        return next_state, stress, done

    def transit(self, state, action, TRIGAR, BRANCH):
        transition_probs = self.transit_func(state, action)
        if len(transition_probs) == 0:
            return None, None, True

        next_states = []
        probs = []
        for s in transition_probs:
            next_states.append(s)
            probs.append(transition_probs[s])

        next_state = np.random.choice(next_states, p=probs)
        stress, done = self.stress_func(next_state, TRIGAR, BRANCH)
        return next_state, stress, done




class Agent():

    def __init__(self, env):
        self.actions = env.actions

    def policy(self, state, TRIGAR, BRANCH, TRIGAR2):
        
        if TRIGAR:
            if BRANCH:
                print("BRANCH TRUE left")
                return (self.actions[2])
            else:
                return (self.actions[1])
        elif not TRIGAR:
            if BRANCH:
                print("BRANCH TRUE right")
                return (self.actions[3])
            else:
                return (self.actions[0])

    def get_distance(x1, y1, x2, y2):
        d = math.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2)
        return d
        


def main():

    
    
    
    NODELIST = [
            [0, 0, 0, 0, 0, 0],
            [0, 0, 0, 0, 0, 0],
            [0, 0, 0, 0, 0, 0],
            [0, 0, 0, 0, 0, 0],
            [0, 0, 1, 0, 0, 0],
            [0, 0, 1, 0, 0, 0],
            [0, 0, 0, 0, 0, 0] # start
    ]

    NODE_COUNT = [
            [0, 0, 0, 0, 0, 0],
            [0, 0, 0, 0, 0, 0],
            [0, 0, 0, 0, 0, 0],
            [0, 0, 0, 0, 0, 0],
            [0, 0, 0, 0, 0, 0],
            [0, 0, 0, 0, 0, 0],
            [0, 0, 0, 0, 0, 0] # start
    ]

    TURNING_POINT = [
            [0, 0, 0, 0, 0, 0],
            [0, 0, 0, 0, 0, 0],
            [0, 0, 0, 0, 0, 0],
            [0, 0, 0, 0, 0, 0],
            [0, 0, 0, 0, 0, 0],
            [0, 0, 0, 0, 0, 0],
            [0, 0, 0, 0, 0, 0] # start
    ]

    # üîë‰ªä„ÅØË¶≥Ê∏¨„Åï„Çå„Å¶„ÅÑ„ÇãÂâçÊèê„ÅÆÁ∞°Âçò„Å™„ÇÑ„Å§
    Observation = [
            [0,   0,   0,   0.8, 0.7, 0],
            [0.4, 0.9, 0.4, 0.5, 0, 0],
            [0.9, 0.5, 0.7, 0.3, 0, 0], # 0.8
            [0.2, 0.8, 0.5, 0, 0, 0],
            [0.6, 0.4, 0.6, 0, 0, 0],
            [0.8, 0.7, 0.3, 0, 0, 0],
            [1,   0,   0,   0, 0, 0] # start
    ]
    print("Observation : {}".format(Observation))
    # 2D grid „Åß„Ç¥„Éº„É´„Å´Ëæø„ÇäÁùÄ„Åè„Å´„ÅØ„ÄÅBPlist„ÇÇÂàÜÂ≤ê„ÅÆÊï∞„Å†„ÅëÂ¢ó„ÇÑ„Åô or ‰∫åÊ¨°ÂÖÉ„Å´„Åó„Å™„ÅÑ„Å®„ÅÑ„Åë„Å™„ÅÑ

    
    
    grid = [
        [0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0]
    ]
    env = Environment(grid, NODELIST)
    agent = Agent(env)



    # „Ç≥„É°„É≥„Éà„Ç¢„Ç¶„Éà0730 -> Stressfull += 1, Stressfull = 1


    # Try 10 game.
    for i in range(1):
        
        # Initialize position of agent.
        state = env.reset()

        ########## parameter ##########
        total_stress = 0
        stress = 1
        done = False
        STATE_HISTORY = []
        TRIGAR = False
        BPLIST = []
        COUNT = 0
        j = 1
        BRANCH = False
        TRIGAR2 = False
        
        print("\n----Init Pose----")
        print(f"State:{state}")
        STATE_HISTORY.append(state)
        print(f"Total Stress:{total_stress}")
        
        action = agent.policy(state, TRIGAR, BRANCH, TRIGAR2)
        next_state, stress, done = env.step(action, TRIGAR, BRANCH)
        prev_state = state # 1„Å§Ââç„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó„Çí‰øùÂ≠ò -> Âæå„Åß„Çπ„Éà„É¨„Çπ„ÅÆÊ∏õÂ∞ë„Å´‰Ωø„ÅÜ
        state = next_state



        while not done:

            print("\n-----{}Steps-----".format(COUNT+1))

            

            

                

            if TRIGAR:
                if state == back_position:
                    BRANCH = True
                    print("=================")
                    print("NEXT ACTION ‚≠êÔ∏è")
                    print("=================")
                    print(f"ü§ñ State:{state}")
                    STATE_HISTORY.append(state)
                    TRIGAR = False
                
                
                
            if NODELIST[state.row][state.column] > 0:
                print("ü™ß NODE : ‚≠ïÔ∏è")
                
            else:
                print("ü™ß NODE : ‚ùå")
                

            if total_stress + stress >= 0:
                total_stress += stress
            
            if total_stress >= 1:
                TRIGAR = True
                print("=================")
                print("FULL ! MAX! üîô‚õîÔ∏è")
                print("=================")
                print(f"ü§ñ State:{state}")
                STATE_HISTORY.append(state)
                # continue

                back_position = prev_state
            else:
                TRIGAR = False

            
            
            
            print(f"ü§ñ State:{state}")
            STATE_HISTORY.append(state)
            print(f"Total Stress:{total_stress}")
            

            action = agent.policy(state, TRIGAR, BRANCH, TRIGAR2)
            next_state, stress, done = env.step(action, TRIGAR, BRANCH)
            prev_state = state # 1„Å§Ââç„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó„Çí‰øùÂ≠ò -> Âæå„Åß„Çπ„Éà„É¨„Çπ„ÅÆÊ∏õÂ∞ë„Å´‰Ωø„ÅÜ
            state = next_state
            
            COUNT += 1
            if COUNT > 10:
                break
            
        
        print("Episode {}: Agent gets {} stress.".format(i, total_stress))
        print("state_history : {}".format(STATE_HISTORY))

if __name__ == "__main__":
    main()
